{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform standard imports:\n",
    "import spacy\n",
    "nlp = spacy.load('ko_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 \t PRON \t 2916077967634857145 \t 나+는\n",
      "오늘 \t NOUN \t 17203112817557272742 \t 오늘\n",
      "달리기를 \t NOUN \t 1386013593075866660 \t 달리+기+를\n",
      "하는 \t VERB \t 7016145617310600694 \t 하+는\n",
      "주자입니다 \t VERB \t 6669919514168566518 \t 주자입니다\n",
      ". \t PUNCT \t 12646065887601541794 \t .\n",
      "달리기를 \t NOUN \t 1386013593075866660 \t 달리+기+를\n",
      "좋아하기 \t VERB \t 10208331703961701902 \t 좋아하+기\n",
      "때문입니다 \t VERB \t 14436976343481477712 \t 때문+이+ㅂ니다\n",
      ". \t PUNCT \t 12646065887601541794 \t .\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u\"나는 오늘 달리기를 하는 주자입니다. 달리기를 좋아하기 때문입니다.\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>In the above sentence, `running`, `run` and `ran` all point to the same lemma `run` (...11841) to avoid duplication.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to display lemmas\n",
    "Since the display above is staggared and hard to read, let's write a function that displays the information we want more neatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're using an **f-string** to format the printed text by setting minimum field widths and adding a left-align to the lemma hash value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는           PRON   2916077967634857145    나+는\n",
      "오늘           NOUN   17203112817557272742   오늘\n",
      "18마리의        NUM    9593124817331040547    18+마리+의\n",
      "생쥐들을         NOUN   11049081752070338458   생쥐+들+을\n",
      "봤어           VERB   16316130839343428056   봤어\n",
      "!            PUNCT  17494803046312582752   !\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"나는 오늘 18마리의 생쥐들을 봤어!\")\n",
    "\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Notice that the lemma of `saw` is `see`, `mice` is the plural form of `mouse`, and yet `eighteen` is its own number, *not* an expanded form of `eight`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내일           NOUN   17486585633085380082   내일\n",
      "미팅에서         ADV    5343797126385849736    미팅+에서\n",
      "그를           PRON   16292183577316820414   그+를\n",
      "만날           VERB   17838407554557962817   만나+ㄹ\n",
      "예정이야         VERB   3092413078387078107    예정+이+야\n",
      "!            PUNCT  17494803046312582752   !\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"내일 미팅에서 그를 만날 예정이야!\")\n",
    "\n",
    "show_lemmas(doc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Here the lemma of `meeting` is determined by its Part of Speech tag.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그것은          PRON   2639474868024230844    그것+은\n",
      "엄청난          ADJ    4138740274229642590    엄청나+ㄴ\n",
      "자동차에요        VERB   4662446312921723792    자동차에요\n",
      ".            PUNCT  12646065887601541794   .\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"그것은 엄청난 자동차에요.\")\n",
    "\n",
    "show_lemmas(doc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Note that lemmatization does *not* reduce words to their most basic synonym - that is, `enormous` doesn't become `big` and `automobile` doesn't become `car`.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should point out that although lemmatization looks at surrounding text to determine a given word's part of speech, it does not categorize phrases. In an upcoming lecture we'll investigate *word vectors and similarity*.\n",
    "\n",
    "## Next up: Stop Words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
